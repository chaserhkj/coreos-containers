ARG RUNTIME_TYPE=rocm

# Prebuilt wheels for cuda platform
# Download prebuilt flash attn from github
FROM alpine/curl as flash-attn-cu
RUN curl -o /flash_attn-2.8.3+cu128torch2.8-cp312-cp312-linux_x86_64.whl \
    https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu128torch2.8-cp312-cp312-linux_x86_64.whl
# Others are built externally, just import them here
FROM localhost/sage-attention-wheel as sage-attn-cu
FROM localhost/sparse-sageattn-2-wheel as spar-sage-attn-cu
# Prebuilt wheels for rocm platform
# Dummies for now
FROM localhost/pytorch-${RUNTIME_TYPE}-venv as flash-attn-rocm
ENV VIRTUAL_ENV=/venv
RUN --mount=type=cache,target=/root/.cache/uv,id=uv \
    uv pip install triton==3.2.0 pip setuptools wheel
ENV FLASH_ATTENTION_TRITON_AMD_ENABLE="TRUE"
RUN git clone https://github.com/ROCm/flash-attention.git &&\ 
    cd flash-attention &&\
    git checkout main_perf &&\
    /venv/bin/pip wheel . --no-deps --no-build-isolation &&\
    cp *.whl /

FROM scratch as sage-attn-rocm
FROM scratch as spar-sage-attn-rocm

# Proxy aliases
FROM flash-attn-${RUNTIME_TYPE} as flash-attn
FROM sage-attn-${RUNTIME_TYPE} as sage-attn
FROM spar-sage-attn-${RUNTIME_TYPE} as spar-sage-attn

FROM localhost/pytorch-${RUNTIME_TYPE}-venv
ARG RUNTIME_TYPE

RUN --mount=type=cache,target=/root/.cache/uv,id=uv bash <<EOF
set -e
. /venv/bin/activate
curl https://raw.githubusercontent.com/comfyanonymous/ComfyUI/refs/heads/master/requirements.txt \
    -o /tmp/comfyui-deps.txt 

if [[ ${RUNTIME_TYPE} == rocm ]]; then
    index_url="https://download.pytorch.org/whl/rocm6.4"
else if [[ ${RUNTIME_TYPE} == cu ]]; then
    index_url="https://download.pytorch.org/whl/cu128"
else
    exit 1
fi
fi

uv pip install -r /tmp/comfyui-deps.txt \
    --extra-index-url \$index_url
uv pip install xformers --index-url \$index_url
EOF

# Install dependencies for various accelerated algorithms
RUN --mount=from=flash-attn,target=/mnt/flash-attn \
    --mount=from=sage-attn,target=/mnt/sage-attn \
    --mount=from=spar-sage-attn,target=/mnt/spar-sage-attn \
    --mount=type=cache,target=/var/cache/apt,id=apt \
    --mount=type=cache,target=/root/.cache/uv,id=uv bash <<EOF
set -e
. /venv/bin/activate

if [[ ${RUNTIME_TYPE} == cu ]]; then
uv pip install /mnt/flash-attn/*.whl \
    /mnt/sage-attn/*.whl \
    /mnt/spar-sage-attn/*.whl \
    git+https://github.com/mit-han-lab/radial-attention
else 
    uv pip install /mnt/flash-attn/*.whl 
    apt-get update && apt-get install -y build-essential python3-dev       
fi
EOF

# For ComfyUI-Manager to manage custom nodes dependencies
RUN --mount=type=cache,target=/root/.cache/uv,id=uv \
    . /venv/bin/activate && uv pip install pip uv

# Other external dependencies required by various custom nodes
RUN --mount=type=cache,target=/var/cache/apt,id=apt \
    apt-get update && apt-get install -y cmake libgl1 ffmpeg

# tini Init proxy
RUN --mount=type=cache,target=/var/cache/apt,id=apt \
    apt-get update && apt-get install -y tini

RUN mkdir /app && chown 1000:1000 -R /venv /app

ENV HOME=/app

USER 1000:1000

ENTRYPOINT ["tini", "--"]

CMD ["/app/entrypoint.sh"]

EXPOSE 8188/tcp